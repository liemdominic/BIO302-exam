---
title: "BIO302 exam"
author: "Liem Oeastraadt-Jennings"
format: html
editor: visual
date: today
editor_options: 
  chunk_output_type: console
---

# Part A - reproducibility

## 1. Continue with your attempt to reproduce part of a paper from archived data from the first practical. Archive the code and text on GitHub and include the link to your repo in your answer here.

Link to repo: https://github.com/liemdominic/BIO302-exam/blob/main/BIO302%20part%20A.qmd

```{r}
# Loading required packages
library(ggplot2) 
library(tidyverse) 
library(readr) 
library(here) 
library(usethis)
```

```{r}
## Importing data coral_cover.csv
coral_cover_data <- read.csv(here("coral_cover.csv")) |>
janitor::clean_names()

view(coral_cover_data)

# Plotting
coral_cover_plot <- ggplot(coral_cover_data, #give the plot a name
       aes(x = site, 
           y = mean_coral_cover, 
           fill = as.factor(year))) + #continous scale issue when year not a factor
  geom_col(position = "dodge") + #to get col's side by side
  labs(x = "Site", 
       y = "Coral Coverage (%)", 
       fill = "Year") +
  scale_fill_manual(values = c("blue", "red")) + #figure in paper had blue and red colours, not exactly sure which shade used.
  theme_bw() +
  scale_x_continuous(breaks = unique(as.numeric(coral_cover_data$site))) #to get a continous scale on x axis

coral_cover_plot
```

I tried to reproduce figure 2c) from the paper Cook et al., 2022. Looking at the data they have published which i downloaded from: https://datadryad.org/stash/dataset/doi:10.5061/dryad.9ghx3ffk7 - I can see that they have cleaned the data a lot, and that only the specific data for the figure is published. The data for the figure is called "coral_cover.csv". I am not sure if the data has been cleaned with code or if it has been cleaned using another data managing program. To try to understand how they had cleaned the data i read the methods (Cook et al., 2022) and readme.txt file, also available for download. According to the methods and readme.txt, all data has been analysed with R, but the data cleaning procedure is not specifically mentioned.

The colour and legend are not the same as in the article figure, but those are reproducible with known choice of colour and legend location coordinates. I did not use unneccessary time trying to reproduce these.

Reproducing the figure went well, but understanding how the data cleaning process was done was not possible (Cook et al., 2022). Understanding the whole figure by itself is not possible, other than understanding that there is a visual difference between mean coral coverage at the times surveyed. Visual surveys by SCUBA diving was the method used and mean percentage coverage across the site-specific quadrants was calculated using scleractinian coral quadrat data. To reproduce the figure completely i would need the coordinates for the offset of legend and the colour scheme they chose. In the data coral_cover.csv, data points are from year 1976 and 2018, but in the figure 2c (Cook et al., 2022), the year's in the legend says: 1975 and 2018. Figure text explains that the visual survey was done in the years 1975-1976 and 2018. Why the legend shows only year 1975 i do not know, as the data shows only year 1976 and 2018. An explenation of this would leave less uncertainty to what this means.

16 data points for 1976 and 16 data points for 2018 were available in the coral_cover.csv file. Data was represented in year, site and mean coral coverage (%). I used janitor::clean_names to format the data column names. I used "here" to create a pathway that does not need Rstudio and creates a file path that locates the file from any other project root. I have commented the steps in my code so that my code is hopefully understandable by others.

Sources:

Cook, K.M. et al. (2022) "A community and functional comparison of coral and reef fish assemblages between four decades of coastal urbanisation and thermal stress," Ecology and evolution, 12(3), pp. e8736-n/a.

# Part B data analysis

## 1. A colleague is testing the effect of a treatment on fish growth. They have ten tanks (five for each treatment). Each tank with ten fish. They plan to fit the model

lm(growth \~ treatment, data = fish_data)

-   What is wrong with this plan?
-   Show, by simulation, the problems with this model for these data.
-   Suggest a better model and show that it performs better.

```{r}
library(ggfortify) # Package needed to run ANOVA tests

# Set seed for reproducibility
set.seed(123)

# Number of tanks and fish per tank
num_tanks_per_treatment <- 5
fish_per_tank <- 10

# Generate treatment labels
Treatment <- rep(c("A", "B"), each = num_tanks_per_treatment * fish_per_tank)

# Generate tank labels
tank <- rep(1:num_tanks_per_treatment, each = fish_per_tank, times = length(Treatment) / (num_tanks_per_treatment * fish_per_tank))

# Simulate random fish growth with varying treatment effects
fish_growth <- rnorm(length(Treatment), mean = ifelse(Treatment == "A", 10, 12), sd = 2)

# Create a data frame
fish_data <- data.frame(Treatment, tank, fish_growth) |>
  janitor::clean_names()

# Checking the lm model
model <- lm(fish_growth ~ Treatment, data = fish_data)

# Print the coefficient estimates
summary(model)$coefficients

# Linear model summary
summary(model)

# Package needed to run mixed effect
library(lme4)

# Fit a mixed-effects model
mixed_model <- lmer(fish_growth ~ treatment + (1 | tank), data = fish_data)

# Print the mixed model summary
summary(mixed_model)

#Running ANOVA tests and autoplot to check goodness of fit
anova(model)

anova(mixed_model)

# Compare model summaries
print(summary(model))
print(summary(mixed_model))

# Plot observed vs. predicted values for both models
lm_model_plot <- ggplot(fish_data, aes(x = fish_growth, y = fitted(model))) +
  geom_point(color = "blue") +
  geom_abline(intercept = 0, slope = 1, color = "red") +
  labs(title = "Observed vs. Predicted (lm Model)")

mixed_model_plot <- ggplot(fish_data, aes(x = fish_growth, y = fitted(mixed_model))) +
  geom_point(color = "green") +
  geom_abline(intercept = 0, slope = 1, color = "red") +
  labs(title = "Observed vs. Predicted (Mixed-Effects Model)")

# Combine the plots using facet_wrap
library(gridExtra)
grid.arrange(lm_model_plot, mixed_model_plot, ncol = 2)

```

## Answer Part B, 1)

Assumptions of the linear model are:

1.  That there is a linear relationship between the dependent variable (in this case fish growth) and the independent variable(s) (treatment).

2.  That the residuals are normal distributed (have a mean of zero).

3.  Has constant variance in residuals (not heteroscedastic).

4.  Residuals are independent (uncorrelated).

The planned model **`lm(growth ~ treatment, data = fish_data)`** assumes that each fish observation is independent of the others, which is not the case in this scenario. The data is structured hierarchically, with fish within tanks, and this violates the independence assumption required for the simple linear regression model (lm) to be valid. When observations are nested within higher-level units (e.g., fish within tanks), the data exhibits a hierarchical or clustered structure, and the standard lm model is not appropriate.

Assumptions of the mixed effect model are mostly the same as the linear model, but with an additional random-effect. The random effect assumes that the intercept follows a normal distribution which in this case would be the effect of the tank, and that the effect is independent of the of the fixed effects. Random effects are included to account for the variability at different levels of grouping (such as the tanks in this case).

To demonstrate the issues with the linear model, l have simulated data with hierarchical structure and then compared the performance of the simple linear regression model with a mixed-effects model that accounts for the tank-level variability.

The comparison of model summaries and the observed vs. predicted plots will show that the simple linear regression model fails to account for the hierarchical structure and leads to biased estimates, whereas the mixed-effects model handles the clustering and provides more accurate predictions.

## 2. You review a paper with the following text.

> "The effect size was small (r = 0.124). However artificially amplifying the sample by a factor of two (from n = 54 to n = 108) yielded a significant result (p = 0.043) suggesting that a future larger study might be able to detect a positive benefit for \[our treatment\]."

What advice do you give the authors?

## Answer part B, 2)

Considering the effect size of r = 0.124, which gives an r\^2 = 0.0154, which means that observed variation is only explained by 1.5% of the dependent variable. The small magnitude of 0.124 indicates that the relationship between the variables is not very strong, and any changes in one variable are associated with only a small change in the other variable.

While a small effect size might be considered weak in some contexts, it could still be meaningful or significant in others, depending on the research area and the practical implications of the findings.

My advice for the author:

1.  Address the small effect size: The author should acknowledge that the effect size from their study is small (r = 0.124). While statistically significant, small effect sizes might have limited practical significance. The author should discuss the potential clinical or real-world implications of such a small effect size and consider whether it would be meaningful to pursue further research on this topic.

2.  Caution regarding artificially amplified sample size: The author should be cautious when interpreting the significant result obtained by artificially amplifying the sample size. While increasing the sample size can increase statistical power and the likelihood of detecting small effects, artificially inflating the data may not reflect the real-world scenarios accurately. The authors should discuss the implications of such an approach and the potential biases it may introduce.

Replication in a larger study: Given the small effect size observed, the author's suggestion of conducting a larger study is reasonable. A larger sample size can provide more robust results and help establish the reliability of the effect. However, the author should also consider modifying their methodology to address any limitations or potential biases identified in the current study.

## 3. The `lynx` data (available with-in R) show the number of lynx (norsk: gaupe) trapped in Canada in 1821-1934. Plot the data then examine the acf and pacf for these data. What can you infer from these about the type of autocorrelation in these data?

## Answer part B, 3)

```{r}
data("lynx")

plot(lynx, 
     type = "l", 
     xlab = "Year", 
     ylab = "Number of Lynx", 
     main = "Lynx Trapped in Canada (1821-1934)")

# ACF plot

acf(lynx, main = "ACF of Lynx Data")

# PACF plot

pacf(lynx, main = "PACF of Lynx Data")
```

Interpretation:

\- ACF (Autocorrelation Function): The ACF plot shows the correlation of the lynx population at different lags (time intervals). If there is a significant autocorrelation at a specific lag, it suggests that the current observation is related to its past values at that lag. In the ACF plot, if there is a slow decay of correlations and it cuts off after a few lags, it indicates a possible AR (AutoRegressive) process.

\- PACF (Partial Autocorrelation Function): The PACF plot shows the correlation between the lynx population at a specific lag and its values at all shorter lags removed. If there is a significant partial autocorrelation at a specific lag, it suggests a direct relationship between the current observation and its past values at that lag. In the PACF plot, if there is a sharp drop after a few lags, it indicates a possible MA (Moving Average) process.

Based on the shapes and patterns observed in the ACF and PACF plots, you can infer the type of autocorrelation present in the lynx data. For instance, if you see significant spikes at certain lags in the ACF plot and a sharp drop after a few lags in the PACF plot, it could suggest an AR(p) process. If you see significant spikes in the PACF plot and a rapid decay in the ACF plot, it could indicate an MA(q) process. If both plots show significant spikes, it could indicate a combination of AR and MA processes, i.e., an ARMA(p,q) process.

Remember that the presence of autocorrelation in time series data is essential for appropriate modeling and forecasting.

## 4. Chironomid species richness has been recorded in some Norwegian lakes. Three predictor variables are available, water temperature, depth and pH. We want to test the hypothesis that species richness is related to temperature.

The data are in the file chironomid.txt.

-   What distribution could be assumed for the response variable?
-   What type of analysis is appropriate?
-   Fit an appropriate parametric model to test the hypothesis.
-   Check the model diagnostics. Justify any changes you need to make to the model.
-   Predict species richness at -5, 5, and 30Â°C and show the 95% confidence intervals.
-   Present the results using both graphs and tables.
-   Write a biological interpretation of your model.

## Instructions

Write in a quarto document and submit the rendered version. Make sure that any figures are included in you submission.

Upload your answer to mitt.uib
